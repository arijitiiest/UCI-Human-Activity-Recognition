{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI Human Action Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import different classifiers\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    data = pd.read_csv(file)\n",
    "    X_data = data.drop(['subject', 'Activity', 'ActivityName'], axis=1)\n",
    "    y_data = data.ActivityName\n",
    "    \n",
    "    return np.array(X_data), np.array(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_x, train_y, model_name='NB', validation=None):\n",
    "    \"\"\"\n",
    "    Possible model names: ['NB', 'SVM', 'XGB', 'MLP', 'ADA', 'BAG', 'RF']\n",
    "    default = 'NB'\n",
    "    \n",
    "    validation: (val_x, val_y) tupple for validation accuracy score.\n",
    "    \n",
    "    return: trained model\n",
    "    \"\"\"\n",
    "    model = None\n",
    "    if model_name == 'SVM':\n",
    "        model = svm.SVC(gamma='scale', probability=True)\n",
    "    elif model_name == 'XGB':\n",
    "        model = XGBClassifier(n_estimators=200, max_depth=5, n_jobs=2)\n",
    "#         model = XGBClassifier()\n",
    "    elif model_name == 'MLP':\n",
    "        model = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=800, alpha=0.0001,\n",
    "                     solver='sgd', verbose=10, tol=0.000000001)\n",
    "    elif model_name == 'ADA':\n",
    "        model = AdaBoostClassifier(n_estimators=50)\n",
    "    elif model_name == 'BAG':\n",
    "        model = BaggingClassifier(n_jobs=2, n_estimators=50)\n",
    "    elif model_name == 'RF':\n",
    "        model = RandomForestClassifier(n_estimators=200, max_depth=10)\n",
    "    elif model_name == 'KNN':\n",
    "        model = KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "    else:\n",
    "        model = GaussianNB()\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "    \n",
    "    if validation is not None:\n",
    "        y_hat = model.predict(validation[0])\n",
    "        acc = metrics.accuracy_score(validation[1], y_hat)\n",
    "        print(f\"Validation Accuracy in '{model_name}' = {acc}\")\n",
    "        cm = metrics.confusion_matrix(validation[1], y_hat)\n",
    "        print(cm)\n",
    "        recall = cm[0][0] / (cm[0][0] + cm[0][1])\n",
    "        precision = cm[0][0] / (cm[0][0] + cm[1][0])\n",
    "        f1 = 2*(precision*recall)/(precision+recall)\n",
    "        print(f\"Recall in '{model_name}' = {recall}\")\n",
    "        print(f\"Precision in '{model_name}' = {precision}\")\n",
    "        print(f\"F1 Score in '{model_name}' = {f1}\")\n",
    "               \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = read_data('data/train.csv')\n",
    "test_X, test_y = read_data('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train  :  (7352, 561) (7352,)\n",
      "Test   :  (2947, 561) (2947,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train  : \", train_X.shape, train_y.shape)\n",
    "print(\"Test   : \", test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51638222, -0.00520684,  0.01300807, ..., -0.56849396,\n",
       "         0.35436829,  0.16933096],\n",
       "       [ 0.16831268,  0.00530207, -0.11588151, ..., -0.82837935,\n",
       "         0.19606057, -0.05194167],\n",
       "       [ 0.25156206,  0.02026222, -0.09302462, ..., -0.69898805,\n",
       "         0.15969038, -0.18949756],\n",
       "       ...,\n",
       "       [ 0.28408296, -0.01934817, -0.11378312, ..., -0.87806504,\n",
       "         0.05640006,  0.09718048],\n",
       "       [ 0.27402938, -0.01563072, -0.10093569, ..., -0.67061088,\n",
       "        -0.01618227, -0.18816923],\n",
       "       [ 0.26942807, -0.01600187, -0.11056543, ...,  0.3938501 ,\n",
       "        -0.72877649,  0.26378415]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy in 'RF' = 0.9239904988123515\n",
      "[[537   0   0   0   0   0]\n",
      " [  0 432  59   0   0   0]\n",
      " [  0  46 486   0   0   0]\n",
      " [  0   0   0 486  10   0]\n",
      " [  0   0   0  25 354  41]\n",
      " [  0   0   0  37   6 428]]\n",
      "Recall in 'RF' = 1.0\n",
      "Precision in 'RF' = 1.0\n",
      "F1 Score in 'RF' = 1.0\n"
     ]
    }
   ],
   "source": [
    "model1 = train_model(train_X, train_y, model_name='RF', validation=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy in 'BAG' = 0.8978622327790974\n",
      "[[537   0   0   0   0   0]\n",
      " [  0 399  92   0   0   0]\n",
      " [  0  54 478   0   0   0]\n",
      " [  0   0   0 477  14   5]\n",
      " [  0   0   0  11 364  45]\n",
      " [  0   0   0  69  11 391]]\n",
      "Recall in 'BAG' = 1.0\n",
      "Precision in 'BAG' = 1.0\n",
      "F1 Score in 'BAG' = 1.0\n"
     ]
    }
   ],
   "source": [
    "model2 = train_model(train_X, train_y, model_name='BAG', validation=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy in 'ADA' = 0.5310485239226331\n",
      "[[537   0   0   0   0   0]\n",
      " [  0   0 491   0   0   0]\n",
      " [  0   0 532   0   0   0]\n",
      " [  0   0   0 496   0   0]\n",
      " [  0   0   0 420   0   0]\n",
      " [  0   0   0 471   0   0]]\n",
      "Recall in 'ADA' = 1.0\n",
      "Precision in 'ADA' = 1.0\n",
      "F1 Score in 'ADA' = 1.0\n"
     ]
    }
   ],
   "source": [
    "model3 = train_model(train_X, train_y, model_name='ADA', validation=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy in 'NB' = 0.7702748557855447\n",
      "[[323 211   0   0   0   3]\n",
      " [  5 368 111   0   0   7]\n",
      " [  8  54 455   0   0  15]\n",
      " [  0   0   0 416  42  38]\n",
      " [  0   0   0  80 257  83]\n",
      " [  0   0   0   9  11 451]]\n",
      "Recall in 'NB' = 0.6048689138576779\n",
      "Precision in 'NB' = 0.9847560975609756\n",
      "F1 Score in 'NB' = 0.7494199535962879\n"
     ]
    }
   ],
   "source": [
    "model4 = train_model(train_X, train_y, model_name='NB', validation=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy in 'SVM' = 0.9504580929759077\n",
      "[[537   0   0   0   0   0]\n",
      " [  0 438  51   0   0   2]\n",
      " [  0  29 503   0   0   0]\n",
      " [  0   0   0 488   3   5]\n",
      " [  0   0   0  10 384  26]\n",
      " [  0   0   0  20   0 451]]\n",
      "Recall in 'SVM' = 1.0\n",
      "Precision in 'SVM' = 1.0\n",
      "F1 Score in 'SVM' = 1.0\n"
     ]
    }
   ],
   "source": [
    "model5 = train_model(train_X, train_y, model_name='SVM', validation=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy in 'XGB' = 0.9426535459789617\n",
      "[[537   0   0   0   0   0]\n",
      " [  0 426  63   0   0   2]\n",
      " [  0  29 503   0   0   0]\n",
      " [  0   0   0 489   3   4]\n",
      " [  0   0   0   8 383  29]\n",
      " [  0   0   0  25   6 440]]\n",
      "Recall in 'XGB' = 1.0\n",
      "Precision in 'XGB' = 1.0\n",
      "F1 Score in 'XGB' = 1.0\n"
     ]
    }
   ],
   "source": [
    "model6 = train_model(train_X, train_y, model_name='XGB', validation=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy in 'KNN' = 0.9002375296912114\n",
      "[[534   2   1   0   0   0]\n",
      " [  0 388 100   0   0   3]\n",
      " [  0  37 495   0   0   0]\n",
      " [  0   0   0 484  10   2]\n",
      " [  0   0   0  44 331  45]\n",
      " [  0   0   0  38  12 421]]\n",
      "Recall in 'KNN' = 0.996268656716418\n",
      "Precision in 'KNN' = 1.0\n",
      "F1 Score in 'KNN' = 0.9981308411214954\n"
     ]
    }
   ],
   "source": [
    "model7 = train_model(train_X, train_y, model_name='KNN', validation=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.69583624\n",
      "Iteration 2, loss = 1.39671824\n",
      "Iteration 3, loss = 1.11764323\n",
      "Iteration 4, loss = 0.90482845\n",
      "Iteration 5, loss = 0.76359330\n",
      "Iteration 6, loss = 0.66750973\n",
      "Iteration 7, loss = 0.59702098\n",
      "Iteration 8, loss = 0.54119112\n",
      "Iteration 9, loss = 0.49391876\n",
      "Iteration 10, loss = 0.45298413\n",
      "Iteration 11, loss = 0.41766312\n",
      "Iteration 12, loss = 0.38774017\n",
      "Iteration 13, loss = 0.36121983\n",
      "Iteration 14, loss = 0.33863531\n",
      "Iteration 15, loss = 0.31845377\n",
      "Iteration 16, loss = 0.30080831\n",
      "Iteration 17, loss = 0.28578848\n",
      "Iteration 18, loss = 0.27197180\n",
      "Iteration 19, loss = 0.25981109\n",
      "Iteration 20, loss = 0.24845548\n",
      "Iteration 21, loss = 0.23768007\n",
      "Iteration 22, loss = 0.22811392\n",
      "Iteration 23, loss = 0.21994205\n",
      "Iteration 24, loss = 0.21169990\n",
      "Iteration 25, loss = 0.20279170\n",
      "Iteration 26, loss = 0.19604339\n",
      "Iteration 27, loss = 0.18974789\n",
      "Iteration 28, loss = 0.18461731\n",
      "Iteration 29, loss = 0.17698515\n",
      "Iteration 30, loss = 0.17225720\n",
      "Iteration 31, loss = 0.16519418\n",
      "Iteration 32, loss = 0.16061557\n",
      "Iteration 33, loss = 0.15521138\n",
      "Iteration 34, loss = 0.15036620\n",
      "Iteration 35, loss = 0.14629362\n",
      "Iteration 36, loss = 0.14100919\n",
      "Iteration 37, loss = 0.13782934\n",
      "Iteration 38, loss = 0.13386486\n",
      "Iteration 39, loss = 0.13050664\n",
      "Iteration 40, loss = 0.12622304\n",
      "Iteration 41, loss = 0.12280610\n",
      "Iteration 42, loss = 0.11935878\n",
      "Iteration 43, loss = 0.11626534\n",
      "Iteration 44, loss = 0.11356996\n",
      "Iteration 45, loss = 0.11177471\n",
      "Iteration 46, loss = 0.10880363\n",
      "Iteration 47, loss = 0.10582355\n",
      "Iteration 48, loss = 0.10455751\n",
      "Iteration 49, loss = 0.10218708\n",
      "Iteration 50, loss = 0.09880622\n",
      "Iteration 51, loss = 0.09734794\n",
      "Iteration 52, loss = 0.09605579\n",
      "Iteration 53, loss = 0.09300259\n",
      "Iteration 54, loss = 0.09184807\n",
      "Iteration 55, loss = 0.08981813\n",
      "Iteration 56, loss = 0.08835964\n",
      "Iteration 57, loss = 0.08658636\n",
      "Iteration 58, loss = 0.08491471\n",
      "Iteration 59, loss = 0.08382543\n",
      "Iteration 60, loss = 0.08219032\n",
      "Iteration 61, loss = 0.08102920\n",
      "Iteration 62, loss = 0.07957701\n",
      "Iteration 63, loss = 0.07952964\n",
      "Iteration 64, loss = 0.07687327\n",
      "Iteration 65, loss = 0.07674877\n",
      "Iteration 66, loss = 0.07519832\n",
      "Iteration 67, loss = 0.07499467\n",
      "Iteration 68, loss = 0.07391888\n",
      "Iteration 69, loss = 0.07164357\n",
      "Iteration 70, loss = 0.07070092\n",
      "Iteration 71, loss = 0.07068186\n",
      "Iteration 72, loss = 0.06931668\n",
      "Iteration 73, loss = 0.06853088\n",
      "Iteration 74, loss = 0.06772469\n",
      "Iteration 75, loss = 0.06609868\n",
      "Iteration 76, loss = 0.06665325\n",
      "Iteration 77, loss = 0.06461911\n",
      "Iteration 78, loss = 0.06382188\n",
      "Iteration 79, loss = 0.06520295\n",
      "Iteration 80, loss = 0.06256018\n",
      "Iteration 81, loss = 0.06249408\n",
      "Iteration 82, loss = 0.06177342\n",
      "Iteration 83, loss = 0.06148040\n",
      "Iteration 84, loss = 0.05957199\n",
      "Iteration 85, loss = 0.05898827\n",
      "Iteration 86, loss = 0.05894332\n",
      "Iteration 87, loss = 0.05896894\n",
      "Iteration 88, loss = 0.05850028\n",
      "Iteration 89, loss = 0.05700386\n",
      "Iteration 90, loss = 0.05665069\n",
      "Iteration 91, loss = 0.05594366\n",
      "Iteration 92, loss = 0.05569107\n",
      "Iteration 93, loss = 0.05563382\n",
      "Iteration 94, loss = 0.05428311\n",
      "Iteration 95, loss = 0.05423471\n",
      "Iteration 96, loss = 0.05305599\n",
      "Iteration 97, loss = 0.05432017\n",
      "Iteration 98, loss = 0.05286324\n",
      "Iteration 99, loss = 0.05240446\n",
      "Iteration 100, loss = 0.05093842\n",
      "Iteration 101, loss = 0.05057659\n",
      "Iteration 102, loss = 0.05090649\n",
      "Iteration 103, loss = 0.05057579\n",
      "Iteration 104, loss = 0.05069694\n",
      "Iteration 105, loss = 0.04949049\n",
      "Iteration 106, loss = 0.04904920\n",
      "Iteration 107, loss = 0.04916335\n",
      "Iteration 108, loss = 0.04849522\n",
      "Iteration 109, loss = 0.04894095\n",
      "Iteration 110, loss = 0.04949309\n",
      "Iteration 111, loss = 0.04842919\n",
      "Iteration 112, loss = 0.04647822\n",
      "Iteration 113, loss = 0.04653363\n",
      "Iteration 114, loss = 0.04689657\n",
      "Iteration 115, loss = 0.04820198\n",
      "Iteration 116, loss = 0.04646697\n",
      "Iteration 117, loss = 0.04537191\n",
      "Iteration 118, loss = 0.04649017\n",
      "Iteration 119, loss = 0.04494215\n",
      "Iteration 120, loss = 0.04484350\n",
      "Iteration 121, loss = 0.04467700\n",
      "Iteration 122, loss = 0.04389538\n",
      "Iteration 123, loss = 0.04317249\n",
      "Iteration 124, loss = 0.04375286\n",
      "Iteration 125, loss = 0.04335790\n",
      "Iteration 126, loss = 0.04381728\n",
      "Iteration 127, loss = 0.04286470\n",
      "Iteration 128, loss = 0.04250697\n",
      "Iteration 129, loss = 0.04377850\n",
      "Iteration 130, loss = 0.04221778\n",
      "Iteration 131, loss = 0.04086735\n",
      "Iteration 132, loss = 0.04081857\n",
      "Iteration 133, loss = 0.04115309\n",
      "Iteration 134, loss = 0.04071034\n",
      "Iteration 135, loss = 0.04063561\n",
      "Iteration 136, loss = 0.03995372\n",
      "Iteration 137, loss = 0.04069351\n",
      "Iteration 138, loss = 0.04127189\n",
      "Iteration 139, loss = 0.04013530\n",
      "Iteration 140, loss = 0.03915363\n",
      "Iteration 141, loss = 0.03874243\n",
      "Iteration 142, loss = 0.03893411\n",
      "Iteration 143, loss = 0.03926008\n",
      "Iteration 144, loss = 0.03892046\n",
      "Iteration 145, loss = 0.03909926\n",
      "Iteration 146, loss = 0.03923797\n",
      "Iteration 147, loss = 0.03765408\n",
      "Iteration 148, loss = 0.03709328\n",
      "Iteration 149, loss = 0.03860808\n",
      "Iteration 150, loss = 0.03862240\n",
      "Iteration 151, loss = 0.03675408\n",
      "Iteration 152, loss = 0.03739732\n",
      "Iteration 153, loss = 0.03753464\n",
      "Iteration 154, loss = 0.03776885\n",
      "Iteration 155, loss = 0.03662589\n",
      "Iteration 156, loss = 0.03594713\n",
      "Iteration 157, loss = 0.03630750\n",
      "Iteration 158, loss = 0.03539756\n",
      "Iteration 159, loss = 0.03784049\n",
      "Iteration 160, loss = 0.03640741\n",
      "Iteration 161, loss = 0.03521172\n",
      "Iteration 162, loss = 0.03492470\n",
      "Iteration 163, loss = 0.03459828\n",
      "Iteration 164, loss = 0.03495591\n",
      "Iteration 165, loss = 0.03486969\n",
      "Iteration 166, loss = 0.03641733\n",
      "Iteration 167, loss = 0.03431547\n",
      "Iteration 168, loss = 0.03357818\n",
      "Iteration 169, loss = 0.03405530\n",
      "Iteration 170, loss = 0.03513729\n",
      "Iteration 171, loss = 0.03357483\n",
      "Iteration 172, loss = 0.03365144\n",
      "Iteration 173, loss = 0.03484754\n",
      "Iteration 174, loss = 0.03288009\n",
      "Iteration 175, loss = 0.03235710\n",
      "Iteration 176, loss = 0.03311375\n",
      "Iteration 177, loss = 0.03473080\n",
      "Iteration 178, loss = 0.03268987\n",
      "Iteration 179, loss = 0.03367408\n",
      "Iteration 180, loss = 0.03200337\n",
      "Iteration 181, loss = 0.03316420\n",
      "Iteration 182, loss = 0.03136089\n",
      "Iteration 183, loss = 0.03236244\n",
      "Iteration 184, loss = 0.03206000\n",
      "Iteration 185, loss = 0.03109557\n",
      "Iteration 186, loss = 0.03284157\n",
      "Iteration 187, loss = 0.03143420\n",
      "Iteration 188, loss = 0.03052148\n",
      "Iteration 189, loss = 0.03055930\n",
      "Iteration 190, loss = 0.03084199\n",
      "Iteration 191, loss = 0.03158976\n",
      "Iteration 192, loss = 0.03000639\n",
      "Iteration 193, loss = 0.03184089\n",
      "Iteration 194, loss = 0.03073431\n",
      "Iteration 195, loss = 0.03074225\n",
      "Iteration 196, loss = 0.03027010\n",
      "Iteration 197, loss = 0.03015526\n",
      "Iteration 198, loss = 0.02872964\n",
      "Iteration 199, loss = 0.03082003\n",
      "Iteration 200, loss = 0.03046612\n",
      "Iteration 201, loss = 0.02953045\n",
      "Iteration 202, loss = 0.03018815\n",
      "Iteration 203, loss = 0.02964665\n",
      "Iteration 204, loss = 0.02903771\n",
      "Iteration 205, loss = 0.03096717\n",
      "Iteration 206, loss = 0.02888296\n",
      "Iteration 207, loss = 0.02983116\n",
      "Iteration 208, loss = 0.02883379\n",
      "Iteration 209, loss = 0.02867923\n",
      "Iteration 210, loss = 0.02821507\n",
      "Iteration 211, loss = 0.02805320\n",
      "Iteration 212, loss = 0.02911599\n",
      "Iteration 213, loss = 0.02822406\n",
      "Iteration 214, loss = 0.02838872\n",
      "Iteration 215, loss = 0.02739202\n",
      "Iteration 216, loss = 0.02704236\n",
      "Iteration 217, loss = 0.02950488\n",
      "Iteration 218, loss = 0.02736915\n",
      "Iteration 219, loss = 0.02762952\n",
      "Iteration 220, loss = 0.02753356\n",
      "Iteration 221, loss = 0.02665048\n",
      "Iteration 222, loss = 0.02742169\n",
      "Iteration 223, loss = 0.02665243\n",
      "Iteration 224, loss = 0.02836317\n",
      "Iteration 225, loss = 0.02649203\n",
      "Iteration 226, loss = 0.02781707\n",
      "Iteration 227, loss = 0.02740461\n",
      "Iteration 228, loss = 0.02601934\n",
      "Iteration 229, loss = 0.02657662\n",
      "Iteration 230, loss = 0.02574643\n",
      "Iteration 231, loss = 0.02684076\n",
      "Iteration 232, loss = 0.02572061\n",
      "Iteration 233, loss = 0.02707399\n",
      "Iteration 234, loss = 0.02665431\n",
      "Iteration 235, loss = 0.02612632\n",
      "Iteration 236, loss = 0.02541837\n",
      "Iteration 237, loss = 0.02669785\n",
      "Iteration 238, loss = 0.02568425\n",
      "Iteration 239, loss = 0.02529145\n",
      "Iteration 240, loss = 0.02520219\n",
      "Iteration 241, loss = 0.02648716\n",
      "Iteration 242, loss = 0.02472795\n",
      "Iteration 243, loss = 0.02672429\n",
      "Iteration 244, loss = 0.02565021\n",
      "Iteration 245, loss = 0.02422464\n",
      "Iteration 246, loss = 0.02607417\n",
      "Iteration 247, loss = 0.02383739\n",
      "Iteration 248, loss = 0.02548306\n",
      "Iteration 249, loss = 0.02606337\n",
      "Iteration 250, loss = 0.02455894\n",
      "Iteration 251, loss = 0.02482897\n",
      "Iteration 252, loss = 0.02552846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.02407212\n",
      "Iteration 254, loss = 0.02458025\n",
      "Iteration 255, loss = 0.02677248\n",
      "Iteration 256, loss = 0.02374630\n",
      "Iteration 257, loss = 0.02385537\n",
      "Iteration 258, loss = 0.02375220\n",
      "Iteration 259, loss = 0.02515390\n",
      "Iteration 260, loss = 0.02304719\n",
      "Iteration 261, loss = 0.02293338\n",
      "Iteration 262, loss = 0.02441084\n",
      "Iteration 263, loss = 0.02421092\n",
      "Iteration 264, loss = 0.02394041\n",
      "Iteration 265, loss = 0.02407972\n",
      "Iteration 266, loss = 0.02297920\n",
      "Iteration 267, loss = 0.02396081\n",
      "Iteration 268, loss = 0.02336860\n",
      "Iteration 269, loss = 0.02357585\n",
      "Iteration 270, loss = 0.02339379\n",
      "Iteration 271, loss = 0.02238554\n",
      "Iteration 272, loss = 0.02246567\n",
      "Iteration 273, loss = 0.02367331\n",
      "Iteration 274, loss = 0.02310994\n",
      "Iteration 275, loss = 0.02194864\n",
      "Iteration 276, loss = 0.02207082\n",
      "Iteration 277, loss = 0.02313145\n",
      "Iteration 278, loss = 0.02339173\n",
      "Iteration 279, loss = 0.02301822\n",
      "Iteration 280, loss = 0.02236818\n",
      "Iteration 281, loss = 0.02303212\n",
      "Iteration 282, loss = 0.02371027\n",
      "Iteration 283, loss = 0.02224689\n",
      "Iteration 284, loss = 0.02269208\n",
      "Iteration 285, loss = 0.02160038\n",
      "Iteration 286, loss = 0.02315875\n",
      "Iteration 287, loss = 0.02320272\n",
      "Iteration 288, loss = 0.02256555\n",
      "Iteration 289, loss = 0.02263908\n",
      "Iteration 290, loss = 0.02273843\n",
      "Iteration 291, loss = 0.02144696\n",
      "Iteration 292, loss = 0.02274833\n",
      "Iteration 293, loss = 0.02228807\n",
      "Iteration 294, loss = 0.02290808\n",
      "Iteration 295, loss = 0.02238002\n",
      "Iteration 296, loss = 0.02141087\n",
      "Iteration 297, loss = 0.02149337\n",
      "Iteration 298, loss = 0.02235517\n",
      "Iteration 299, loss = 0.02250300\n",
      "Iteration 300, loss = 0.02126719\n",
      "Iteration 301, loss = 0.02061342\n",
      "Iteration 302, loss = 0.02056593\n",
      "Iteration 303, loss = 0.02215497\n",
      "Iteration 304, loss = 0.01998529\n",
      "Iteration 305, loss = 0.02135013\n",
      "Iteration 306, loss = 0.01991841\n",
      "Iteration 307, loss = 0.02225448\n",
      "Iteration 308, loss = 0.02167712\n",
      "Iteration 309, loss = 0.02078627\n",
      "Iteration 310, loss = 0.01980190\n",
      "Iteration 311, loss = 0.01998243\n",
      "Iteration 312, loss = 0.01974526\n",
      "Iteration 313, loss = 0.02123793\n",
      "Iteration 314, loss = 0.01997886\n",
      "Iteration 315, loss = 0.02078385\n",
      "Iteration 316, loss = 0.02053196\n",
      "Iteration 317, loss = 0.01946899\n",
      "Iteration 318, loss = 0.02168079\n",
      "Iteration 319, loss = 0.02043648\n",
      "Iteration 320, loss = 0.02055036\n",
      "Iteration 321, loss = 0.01980121\n",
      "Iteration 322, loss = 0.01852104\n",
      "Iteration 323, loss = 0.02068125\n",
      "Iteration 324, loss = 0.02061110\n",
      "Iteration 325, loss = 0.01951925\n",
      "Iteration 326, loss = 0.01861034\n",
      "Iteration 327, loss = 0.01896492\n",
      "Iteration 328, loss = 0.01854223\n",
      "Iteration 329, loss = 0.01900895\n",
      "Iteration 330, loss = 0.01824771\n",
      "Iteration 331, loss = 0.01887220\n",
      "Iteration 332, loss = 0.01796151\n",
      "Iteration 333, loss = 0.01844958\n",
      "Iteration 334, loss = 0.01807456\n",
      "Iteration 335, loss = 0.01968255\n",
      "Iteration 336, loss = 0.01911210\n",
      "Iteration 337, loss = 0.01928287\n",
      "Iteration 338, loss = 0.01867443\n",
      "Iteration 339, loss = 0.01774598\n",
      "Iteration 340, loss = 0.01879380\n",
      "Iteration 341, loss = 0.01952282\n",
      "Iteration 342, loss = 0.01931425\n",
      "Iteration 343, loss = 0.01838863\n",
      "Iteration 344, loss = 0.01820122\n",
      "Iteration 345, loss = 0.01858031\n",
      "Iteration 346, loss = 0.01747063\n",
      "Iteration 347, loss = 0.01810240\n",
      "Iteration 348, loss = 0.01842942\n",
      "Iteration 349, loss = 0.01881416\n",
      "Iteration 350, loss = 0.01750067\n",
      "Iteration 351, loss = 0.01842487\n",
      "Iteration 352, loss = 0.01831861\n",
      "Iteration 353, loss = 0.01765095\n",
      "Iteration 354, loss = 0.01839126\n",
      "Iteration 355, loss = 0.01692014\n",
      "Iteration 356, loss = 0.01678006\n",
      "Iteration 357, loss = 0.01737255\n",
      "Iteration 358, loss = 0.01645630\n",
      "Iteration 359, loss = 0.01719188\n",
      "Iteration 360, loss = 0.01837133\n",
      "Iteration 361, loss = 0.01687546\n",
      "Iteration 362, loss = 0.01679827\n",
      "Iteration 363, loss = 0.01715019\n",
      "Iteration 364, loss = 0.01680083\n",
      "Iteration 365, loss = 0.01771297\n",
      "Iteration 366, loss = 0.01831605\n",
      "Iteration 367, loss = 0.01732781\n",
      "Iteration 368, loss = 0.01695961\n",
      "Iteration 369, loss = 0.01708782\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "Validation Accuracy in 'MLP' = 0.9579233118425518\n",
      "[[537   0   0   0   0   0]\n",
      " [  0 433  55   0   0   3]\n",
      " [  0   9 522   1   0   0]\n",
      " [  0   0   0 490   4   2]\n",
      " [  0   0   0   4 392  24]\n",
      " [  0   0   0  21   1 449]]\n",
      "Recall in 'MLP' = 1.0\n",
      "Precision in 'MLP' = 1.0\n",
      "F1 Score in 'MLP' = 1.0\n"
     ]
    }
   ],
   "source": [
    "model8 = train_model(train_X, train_y, model_name='MLP', validation=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
